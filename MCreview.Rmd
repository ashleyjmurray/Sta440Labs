---
title: "Refresher on Monte Carlo Methods"
author: "David Buch"
date: "8/19/2020"
output: ioslides_presentation
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(R2jags)
```

## Basic Visualizations for EDA

```{r}
descrim <- read.csv("data/cal_descrimination.csv", stringsAsFactors = T)
summary(descrim)
plot(descrim[,c("Age","Age.Cohort", "Expenditures")])
coplot(Expenditures ~ Age | Ethnicity, panel = panel.smooth, data = descrim)
boxplot(Expenditures ~ Gender, data = descrim)
```

Also good to learn **ggplot2** and **lattice** packages.


## Basic Model Fitting



## Bayesian Model Fitting a.k.a. Posterior Computation
0. Fully Conjugate Models
1. Writing Custom Gibbs/MH Samplers
2. Using JAGS
3. Using Stan

## Fully Conjugate

Advantages: 

1. Fastest 
2. No ambiguity in convergence

Disadvantages: 

1. (Usually) only available for simple models

Example: Normal-Inverse Gamma prior for mean and variance of normal data

## Writing Custom Gibbs/MH Samplers

Advantages: 

1. Full customization lets you implement model-specific speed-ups
2. No reliance on black-box methods

Disadvantages: 

1. Code can be slower or have errors
2. (Often) has to be written new for each project

Example: Half-normal priors for variance of normal data

## Using JAGS

Advantages: 

1. Once you are familiar with syntax, writting down models is easy. 
2. Pretty fast for some models

Disadvantages: 

1. Need to become familiar with syntax
2. Pretty slow for some model

## JAGS Example - Model

$$Y_i = \sum_j \beta_jX_{ij} + \epsilon_i, \ \epsilon \sim N(0,\sigma^2) \implies Y_i \sim N\left(\sum_j \beta_jX_{ij},\sigma^2\right)$$
Prior: 

$$\boldsymbol{\beta}|\sigma^2 \sim MVN(\mathbf{0},n \sigma^2 (X^T X)^{-1}), \ \sigma^{-2} \sim Gam(1/2,1/2) $$



## JAGS Example - Model Syntax

```{r, cache=T,warning=F,echo=T}
model <- function(){
  for(i in 1:n){
    y[i] ~ dnorm(mu[i],tau)
    mu[i] = inprod(X[i,1:p],beta) 
    resid[i] = y[i] - mu[i]
  }
  
  m0 = rep(0,p)
  
  beta ~ dmnorm(m0,tau*(t(X) %*% X)/n)
  tau = pow(sigma2,-1)
  sigma = pow(sigma2,1/2)
  sigma2 ~ dgamma(1/2,1/2)
}
```

## JAGS Example - Fitting the Model

```{r, echo=T,warning=F, results='hide'}
descrim.modelmat <- 
  model.matrix(Expenditures ~ Age.Cohort + Gender + Ethnicity,
               data = descrim)
jags.result.notconverged <- 
  jags(data = list(y=descrim$Expenditures,X=descrim.modelmat,
                   n=nrow(descrim.modelmat),p=ncol(descrim.modelmat)),
       model.file = model,parameters.to.save = c("beta","sigma"),
       n.iter = 1000,n.burnin = 0)
jags.result.converged <- 
  jags(data = list(y=descrim$Expenditures,X=descrim.modelmat,
                   n=nrow(descrim.modelmat),p=ncol(descrim.modelmat)),
       model.file = model,parameters.to.save = c("beta","sigma"),
       n.iter = 1000)
```


## Using Stan

Advantages: 

1. Basically the same as JAGS, but faster/slower for some models.  
2. Has automatic warnings/checks for posterior convergence

Disadvantages: 

1. Cannot sample discrete parameters
2. Another model syntax to learn

## What do we do with all those simulations?
1. Run Diagnostics (for *Markov Chain* MC)
2. Run Diagnostics (for Model)
3. Calculate Interesting Quantities!


## MCMC Diagnostics

Traceplots

Lag-1 Scatter plots

ACF plots

Rhat/ESS

```{r}
sigma_nc <- jags.result.notconverged$BUGSoutput$sims.array[,1,16]
sigma_c <- jags.result.converged$BUGSoutput$sims.array[,1,16]
sigma.data <- rbind(tibble(sigma=sigma_nc,converged=FALSE,iteration = 1:length(sigma_nc)),
                    tibble(sigma=sigma_c,converged=TRUE,iteration = 1:length(sigma_c)))
```

## MCMC Diagnostics - Traceplots

```{r}
sigma.data %>% 
  ggplot(aes(y=sigma,x=iteration,color = converged)) + 
  geom_line()
```

## MCMC Diagnostics - Lag-1 Scatter Plots

```{r, warning = F}
sigma.data %>% group_by(converged) %>% 
  mutate(sigmat1 = lag(sigma,1),
         converged = ifelse(converged,"Converged","Not Converged")) %>% 
  ggplot(aes(y=sigma,x=sigmat1)) + 
  geom_point() + 
  facet_wrap(~converged,scales = "free") + 
  xlab(latex2exp::TeX("$\\sigma_{t-1}$")) +
  ylab(latex2exp::TeX("$\\sigma_{t}$")) 
```

## MCMC Diagnostics - ACF Plots

```{r}
par(mfrow = c(1,2))
acf(sigma.data$sigma[sigma.data$converged],main = "Converged ACF Plot")
acf(sigma.data$sigma[!sigma.data$converged],main = "Not Converged ACF Plot")
par(mfrow = c(1,1))
```

## MCMC Diagnostics - Rhat/ESS

```{r,echo=T}
jags.result.converged$BUGSoutput$summary[,c(1:3,8:9)]
jags.result.notconverged$BUGSoutput$summary[,c(1:3,8:9)]
```

## MCMC Diagnostics - Rhat/ESS

```{r,echo=T}
jags.result.notconverged$BUGSoutput$summary[,c(1:3,8:9)]
```

## Model Diagnostics

posterior predictive distribution

QQ-plots for regression

Residuals vs fitted values

## Posterior Inference
One of the big payoffs of Bayesian inference and especially Monte Carlo computation is straightforward calculation of interpretable inferential quantities. In the typical frequentist setting there can annoying computational and interpretational challenges in estimating, say, $\Pr(\alpha > 3\beta + \gamma )$, but this is no problem when we've simulated a large number of draws from the joint posterior distribution of our parameters.

