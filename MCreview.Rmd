---
title: "Refresher on Monte Carlo Methods"
author: "Graham and David"
date: "8/19/2020"
output: ioslides_presentation
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(R2jags)
library(rstan)
```

## Basic Visualizations for EDA

```{r, echo = T}
descrim <- read.csv("data/cal_descrimination.csv", stringsAsFactors = T)
par(mfrow = c(2,2))
plot(descrim[,c("Age","Age.Cohort", "Expenditures")])
coplot(Expenditures ~ Age | Ethnicity, panel = panel.smooth, data = descrim)
boxplot(Expenditures ~ Gender, data = descrim)
par(mfrow = c(1,1))
```

Also good to learn **ggplot2** and **lattice** packages.


## Basic Model Fitting
```{r, echo = T}
flmod <- lm(Expenditures ~ Age.Cohort + Gender + Ethnicity,
            data = descrim)
summary(flmod)
```


## Bayesian Model Fitting a.k.a. Posterior Computation
0. Fully Conjugate Models
1. Writing Custom Gibbs/MH Samplers
2. Using JAGS
3. Using Stan

## Fully Conjugate

Advantages: 

1. Fastest 
2. No ambiguity in convergence

Disadvantages: 

1. (Usually) only available for simple models

Example: Normal-Inverse Gamma prior for mean and variance of normal data

## Writing Custom Gibbs/MH Samplers

Advantages: 

1. Full customization lets you implement model-specific speed-ups
2. No reliance on black-box methods

Disadvantages: 

1. Code can be slower or have errors
2. (Often) has to be written new for each project

Example: Half-normal priors for variance of normal data

## Using JAGS

Advantages: 

1. Once you are familiar with syntax, writting down models is easy. 
2. Pretty fast for some models

Disadvantages: 

1. Need to become familiar with syntax
2. Pretty slow for some model

## JAGS Example - Model

$$Y_i = \sum_j \beta_jX_{ij} + \epsilon_i, \ \epsilon \sim N(0,\sigma^2) \implies Y_i \sim N\left(\sum_j \beta_jX_{ij},\sigma^2\right)$$
Prior: 

$$\boldsymbol{\beta}|\sigma^2 \sim MVN(\mathbf{0},n \sigma^2 (X^T X)^{-1}), \ \sigma^{-2} \sim Gam(1/2,1/2) $$



## JAGS Example - Model Syntax

```{r, cache=T,warning=F,echo=T}
model <- function(){
  for(i in 1:n){
    y[i] ~ dnorm(mu[i],tau)
    mu[i] = inprod(X[i,1:p],beta) 
    resid[i] = y[i] - mu[i]
  }
  
  m0 = rep(0,p)
  
  beta ~ dmnorm(m0,tau*(t(X) %*% X)/n)
  tau = pow(sigma2,-1)
  sigma = pow(sigma2,1/2)
  sigma2 ~ dgamma(1/2,1/2)
}
```

## JAGS Example - Fitting the Model

```{r, echo=T,warning=F, results='hide'}
descrim.modelmat <- 
  model.matrix(Expenditures ~ Age.Cohort + Gender + Ethnicity,
               data = descrim)
jags.result.notconverged <- 
  jags(data = list(y=descrim$Expenditures,X=descrim.modelmat,
                   n=nrow(descrim.modelmat),p=ncol(descrim.modelmat)),
       model.file = model,parameters.to.save = c("beta","sigma"),
       n.iter = 1000,n.burnin = 0)
jags.result.converged <- 
  jags(data = list(y=descrim$Expenditures,X=descrim.modelmat,
                   n=nrow(descrim.modelmat),p=ncol(descrim.modelmat)),
       model.file = model,parameters.to.save = c("beta","sigma"),
       n.iter = 1000)
```


## Using Stan

Advantages: 

1. Basically the same as JAGS, but faster/slower for some models.  
2. Has automatic warnings/checks for posterior convergence

Disadvantages: 

1. Cannot sample discrete parameters
2. Another model syntax to learn

## Stan Example
```{r, cache=T,warning=F,echo=T}
model_text <- "
data {
  int<lower=0> N;  // Number of observations
  int<lower=0> P;  // Number of predictors
  matrix[N,P] x;  // Covariate data
  vector[N] y;    // Response
}
parameters {
  vector[P] beta; // regression coeffs
  real<lower=0> sigma; // sd of error term
}
model {
  y ~ normal(x * beta, sigma); // sd parameterization
}
"
```

## Stan Example -- Fit
```{r, cache=T,message=F,warning=F,error=F,echo=T, results='hide'}
model_data <- list(y = descrim$Expenditures, x = descrim.modelmat, 
                   N = nrow(descrim.modelmat), 
                   P = ncol(descrim.modelmat))
stanfit <- stan(model_code = model_text, data = model_data)
```

```{r,echo = T}
colnames(descrim.modelmat)
stanfit
```



## What do we do with all those simulations?
1. Run Diagnostics (for *Markov Chain* MC)
2. Run Diagnostics (for Model)
3. Calculate Interesting Quantities!


## MCMC Diagnostics

Traceplots

Lag-1 Scatter plots

ACF plots

Rhat/ESS

```{r}
sigma_nc <- jags.result.notconverged$BUGSoutput$sims.array[,1,16]
sigma_c <- jags.result.converged$BUGSoutput$sims.array[,1,16]
sigma.data <- rbind(tibble(sigma=sigma_nc,converged=FALSE,iteration = 1:length(sigma_nc)),
                    tibble(sigma=sigma_c,converged=TRUE,iteration = 1:length(sigma_c)))
```

## MCMC Diagnostics - Traceplots

```{r}
sigma.data %>% 
  ggplot(aes(y=sigma,x=iteration,color = converged)) + 
  geom_line()
```

## MCMC Diagnostics - Lag-1 Scatter Plots

```{r, warning = F}
converge_labs <- c("Converged", "Not Converged")
sigma.data %>% group_by(converged) %>% 
  mutate(sigmat1 = lag(sigma,1)) %>% 
  ggplot(aes(y=sigma,x=sigmat1)) + 
  geom_point() + 
  facet_wrap(~converged,scales = "free") + 
  xlab(latex2exp::TeX("$\\sigma_{t-1}$")) +
  ylab(latex2exp::TeX("$\\sigma_{t}$")) 
```

## MCMC Diagnostics - ACF Plots

```{r}
par(mfrow = c(1,2))
acf(sigma.data$sigma[sigma.data$converged],main = "Converged ACF Plot")
acf(sigma.data$sigma[!sigma.data$converged],main = "Not Converged ACF Plot")
par(mfrow = c(1,1))
```

## MCMC Diagnostics - Rhat/ESS

```{r,echo=T}
jags.result.converged$BUGSoutput$summary[,c(1:3,8:9)]
jags.result.notconverged$BUGSoutput$summary[,c(1:3,8:9)]
```

## MCMC Diagnostics - Rhat/ESS

```{r,echo=T}
jags.result.notconverged$BUGSoutput$summary[,c(1:3,8:9)]
```

## Model Diagnostics

posterior predictive distribution

QQ-plots for regression

Residuals vs fitted values

## Model Diagnostics - Posterior Predictive
>"The idea behind posterior predictive checking is simple: if a model is a good fit then we should be able to use it to generate data that looks a lot like the data we observed." 
--Jonah Gabry in "Graphical posterior predictive checks using the bayesplot package"

https://mc-stan.org/bayesplot/articles/graphical-ppcs.html

```{r}
stansamps <- extract(stanfit)
## Generate artificial data below many times
K <- 10
ysims <- matrix(nrow = nrow(descrim.modelmat), ncol = K)
for(k in 1:K){
  for(i in 1:nrow(descrim.modelmat)){
    w <- sample(1:nrow(stansamps$beta),1)
    ysims[i,k] <-  rnorm(1,mean = descrim.modelmat[i,]*stansamps$beta[w,], sd = stansamps$sigma[w])
  }
}

r <- sample(1:nrow(descrim),2)
hist(ysims[r,])
abline(v = descrim$Expenditures[r], col = "red")
print(mean(ysims < 0))
print(mean(descrim$Expenditures < 0))
```

## Model Diagnostics - QQ-plots
```{r}
beta_means <- colMeans(stansamps$beta)
lin_fit <- descrim.modelmat %*% beta_means
res <- descrim$Expenditures - lin_fit
qqnorm(res/sd(res))
abline(a = 0, b = 1, col= "red")
```


## Model Diagnostics - Residuals vs Fitted Values
```{r}
plot(lin_fit, res, ylab = "Residuals", xlab = "Fitted Values")
```


## Posterior Inference
One of the big payoffs of Bayesian inference and especially Monte Carlo computation is straightforward calculation of interpretable inferential quantities. In the typical frequentist setting there can annoying computational and interpretational challenges in estimating, say, $\Pr(\alpha > 3\beta + \gamma )$, but this is no problem when we've simulated a large number of draws from the joint posterior distribution of our parameters.

## Posterior Inference
Once we have fit a model that we are satisfied with, and have performed basic checks for MCMC posterior convergence, we can proceed with inference tasks. Using our posterior samples from Stan or JAGS, and assuming we were satisfied with our model, let's calculate:

1. A 95\% CI for the difference in expenditure for Males vs. Females (of referent ethnicity and age cohort, if you included interactions)
2. Probability that a person who identifies as Multi Race receives less expenditure than a person who is Hispanic
3. The probability that the standard error of the residuals (sigma) is greater than 4000.
